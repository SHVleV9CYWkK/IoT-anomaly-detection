{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Import necessary dependencies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7815af7af69f60e"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:20:15.961889Z",
     "start_time": "2023-11-19T12:20:14.660053Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:20:15.967966Z",
     "start_time": "2023-11-19T12:20:15.963172Z"
    }
   },
   "id": "12b79a6c06694f92"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:20:15.968576Z",
     "start_time": "2023-11-19T12:20:15.965685Z"
    }
   },
   "id": "5a088d0e8dead3f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a59fabd441d22859"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data = pd.read_csv('IoT_Modbus.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:20:16.189271Z",
     "start_time": "2023-11-19T12:20:16.060082Z"
    }
   },
   "id": "969c8f8886dbef26"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO: Complete EDA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aee1e602ca5c766b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Combine 'date' and 'time' into a single datetime column"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34a9dccfe7b750f4"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "data['date'] = data['date'].str.strip()\n",
    "data['time'] = data['time'].str.strip()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:20:16.657867Z",
     "start_time": "2023-11-19T12:20:16.547466Z"
    }
   },
   "id": "21a61b6e743a0aef"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data['datetime'] = pd.to_datetime(data['date'] + ' ' + data['time'], format='%d-%b-%y %H:%M:%S')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:20:17.090743Z",
     "start_time": "2023-11-19T12:20:16.709803Z"
    }
   },
   "id": "b0c2e5922a26caa1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract time features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5a533a2b1f01a79"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "data['year'] = data['datetime'].dt.year\n",
    "data['month'] = data['datetime'].dt.month\n",
    "data['day'] = data['datetime'].dt.day\n",
    "data['hour'] = data['datetime'].dt.hour\n",
    "data['minute'] = data['datetime'].dt.minute\n",
    "data['second'] = data['datetime'].dt.second\n",
    "data['dayofweek'] = data['datetime'].dt.dayofweek"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:20:17.123574Z",
     "start_time": "2023-11-19T12:20:17.092288Z"
    }
   },
   "id": "c0beb939e4303617"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Time series models need to ensure that the data set is arranged in time order"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfdf8be0feb86374"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Sort the data by datetime\n",
    "data = data.sort_values(by='datetime')\n",
    "\n",
    "# Drop the original date, time, and timestamp columns\n",
    "data.drop(['date', 'time', 'datetime', 'type'], axis=1, inplace=True)\n",
    "\n",
    "# Adjust feature order\n",
    "order = ['year', 'month', 'day', 'hour', 'minute', 'second', 'dayofweek', 'FC1_Read_Input_Register', 'FC2_Read_Discrete_Value', 'FC3_Read_Holding_Register', 'FC4_Read_Coil', 'label']\n",
    "data = data[order].astype('int32')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:20:17.376104Z",
     "start_time": "2023-11-19T12:20:17.325958Z"
    }
   },
   "id": "87cbc212dbe19442"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split the dataset (Sequential Split)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "347aaeb8bf6dd766"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Calculate split points\n",
    "split_idx = int(len(data) * 0.8)\n",
    "\n",
    "# Split the data set, keeping order\n",
    "train_data = data.iloc[:split_idx]\n",
    "test_data = data.iloc[split_idx:]\n",
    "\n",
    "train_data_copy = train_data.copy()\n",
    "\n",
    "split_idx_train_validate = int(len(train_data_copy) * 0.8)\n",
    "\n",
    "# Training data is the first 80%\n",
    "train_data = train_data_copy.iloc[:split_idx_train_validate]\n",
    "# Validation data is the last 20%\n",
    "validation_data = train_data_copy.iloc[split_idx_train_validate:]\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_data.drop('label', axis=1)\n",
    "y_train = train_data['label']\n",
    "\n",
    "X_validate = validation_data.drop('label', axis=1)\n",
    "y_validate = validation_data['label']\n",
    "\n",
    "X_test = test_data.drop('label', axis=1)\n",
    "y_test = test_data['label']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:20:17.722479Z",
     "start_time": "2023-11-19T12:20:17.709007Z"
    }
   },
   "id": "7f9a3e7015aaf6cb"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['year', 'month', 'day', 'hour', 'minute', 'second', 'dayofweek',\n       'FC1_Read_Input_Register', 'FC2_Read_Discrete_Value',\n       'FC3_Read_Holding_Register', 'FC4_Read_Coil'],\n      dtype='object')"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:20:17.885247Z",
     "start_time": "2023-11-19T12:20:17.878883Z"
    }
   },
   "id": "be6d0b72401971b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preprocessing (Normalization)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17f2fffdb83cab7f"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 183804 entries, 541 to 149866\n",
      "Data columns (total 11 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   year                       183804 non-null  float32\n",
      " 1   month                      183804 non-null  float32\n",
      " 2   day                        183804 non-null  float32\n",
      " 3   hour                       183804 non-null  float32\n",
      " 4   minute                     183804 non-null  float32\n",
      " 5   second                     183804 non-null  float32\n",
      " 6   dayofweek                  183804 non-null  float32\n",
      " 7   FC1_Read_Input_Register    183804 non-null  float32\n",
      " 8   FC2_Read_Discrete_Value    183804 non-null  float32\n",
      " 9   FC3_Read_Holding_Register  183804 non-null  float32\n",
      " 10  FC4_Read_Coil              183804 non-null  float32\n",
      "dtypes: float32(11)\n",
      "memory usage: 9.1 MB\n"
     ]
    }
   ],
   "source": [
    "feature_columns = X_train.columns\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train[feature_columns] = scaler.fit_transform(X_train[feature_columns]).astype('float32')\n",
    "X_validate[feature_columns] = scaler.fit_transform(X_validate[feature_columns]).astype('float32')\n",
    "X_test[feature_columns] = scaler.transform(X_test[feature_columns]).astype('float32')\n",
    "X_train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:20:18.817112Z",
     "start_time": "2023-11-19T12:20:18.780770Z"
    }
   },
   "id": "485e69a0e89e7b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execution model\n",
    "## Create model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25c12c1b7fb50ea4"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class LightweightLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, width_multiplier=1.0):\n",
    "        super(LightweightLSTM, self).__init__()\n",
    "        # Adjust hidden size based on the width multiplier\n",
    "        adjusted_hidden_size = int(hidden_size * width_multiplier)\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, adjusted_hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        self.linear_1 = nn.Linear(adjusted_hidden_size, hidden_size)\n",
    "        self.linear_2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # Take the output of the last time step\n",
    "        last_time_step_out = lstm_out[:, -1, :]\n",
    "\n",
    "        # Output layer\n",
    "        x = self.linear_1(last_time_step_out)\n",
    "        out = self.linear_2(x)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:20:20.047745Z",
     "start_time": "2023-11-19T12:20:20.036674Z"
    }
   },
   "id": "2ef4fe25ff764a21"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3417f767ae1b5015"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "features_num = X_train.shape[1]\n",
    "hidden_neurons_num = 512\n",
    "output_neurons_num = 1\n",
    "lstm_num_layers = 2\n",
    "multiplier = 0.5\n",
    "\n",
    "temp_model = LightweightLSTM(features_num, hidden_neurons_num, output_neurons_num, lstm_num_layers, multiplier).to(device)\n",
    "model = torch.compile(temp_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:20:21.697680Z",
     "start_time": "2023-11-19T12:20:21.510015Z"
    }
   },
   "id": "2d66654954854fc2"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:20:21.967513Z",
     "start_time": "2023-11-19T12:20:21.947479Z"
    }
   },
   "id": "5dd5b70589db43fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build loss functions and optimizers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9da11cc42cf71b4f"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "weights = torch.tensor([1, class_weights[1]], dtype=torch.float)\n",
    "criterion = nn.BCEWithLogitsLoss(torch.FloatTensor([weights[1] / weights[0]])).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:20:22.759059Z",
     "start_time": "2023-11-19T12:20:22.740932Z"
    }
   },
   "id": "f491c38526ef20b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Construct Data Loader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b12e726d70842c15"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# Use to_numpy is much better than .values\n",
    "X_train_tensor: torch.Tensor = torch.tensor(X_train.to_numpy()).float().unsqueeze(1).to(device)\n",
    "X_validate_tensor: torch.Tensor = torch.tensor(X_validate.to_numpy()).float().unsqueeze(1).to(device)\n",
    "\n",
    "y_train_tensor: torch.Tensor = torch.tensor(y_train.to_numpy()).float().unsqueeze(1).to(device)\n",
    "y_validate_tensor: torch.Tensor = torch.tensor(y_validate.to_numpy()).float().unsqueeze(1).to(device)\n",
    "\n",
    "\n",
    "train_dataset: torch.utils.data.dataset.TensorDataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader: torch.utils.data.dataloader.DataLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "validation_dataset: torch.utils.data.dataset.TensorDataset = TensorDataset(X_validate_tensor, y_validate_tensor)\n",
    "validation_loader: torch.utils.data.dataloader.DataLoader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:20:23.702390Z",
     "start_time": "2023-11-19T12:20:23.691882Z"
    }
   },
   "id": "578fb1053dfb20ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "677881d18ae0a8bf"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 27\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m inputs, labels \u001B[38;5;129;01min\u001B[39;00m validation_loader:\n\u001B[0;32m---> 27\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m(inputs)\n\u001B[1;32m     29\u001B[0m         probabilities \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msigmoid(outputs)\n\u001B[1;32m     31\u001B[0m         loss \u001B[38;5;241m=\u001B[39m criterion(probabilities, labels)\n",
      "Cell \u001B[0;32mIn[17], line 27\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m inputs, labels \u001B[38;5;129;01min\u001B[39;00m validation_loader:\n\u001B[0;32m---> 27\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m(inputs)\n\u001B[1;32m     29\u001B[0m         probabilities \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msigmoid(outputs)\n\u001B[1;32m     31\u001B[0m         loss \u001B[38;5;241m=\u001B[39m criterion(probabilities, labels)\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Applications/DataSpell.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/DataSpell.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "pbar = tqdm(total=num_epochs)\n",
    "loss_list = [None] * num_epochs\n",
    "acc_list = [None] * num_epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:    \n",
    "        # FP\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # BP and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate indicators\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            \n",
    "            loss = criterion(probabilities, labels)\n",
    "            \n",
    "            # Calculate indicators\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            predictions = (probabilities > 0.5).float().to(device)\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(validation_loader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    loss_list[epoch] = epoch_loss\n",
    "    acc_list[epoch] = accuracy\n",
    "    \n",
    "    pbar.update(1)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss}, Accuracy: {accuracy}')\n",
    "\n",
    "pbar.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:21:01.756745Z",
     "start_time": "2023-11-19T12:20:29.737111Z"
    }
   },
   "id": "234ca3f008e97e4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualizing the training process"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a90b287ae5db1a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_list, label='Training Loss')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Draw accuracy curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(acc_list, label='Training Accuracy')\n",
    "plt.title('Training Accuracy per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-19T12:15:43.020460Z"
    }
   },
   "id": "7bab09ae6142362b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unseen test set performance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74bc101f20737968"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test.values).float().unsqueeze(1).to(device)\n",
    "\n",
    "model.eval()\n",
    "outputs = model(X_test_tensor)\n",
    "with torch.no_grad():\n",
    "    probabilities = torch.sigmoid(outputs)\n",
    "    predictions = (probabilities > 0.5).float().cpu().numpy()\n",
    "\n",
    "    # Calculate indicators\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "\n",
    "    print(\"Accuracy: \", acc, \", Precision: \", precision, \", Recall: \", recall, \", F1: \", f1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:15:43.024862Z",
     "start_time": "2023-11-19T12:15:43.021412Z"
    }
   },
   "id": "ff3af552cbe05e41"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a37bb5c5380a6d9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "model_filename = f\"model_{current_time}.pt\"\n",
    "torch.save(model.state_dict(), model_filename)\n",
    "\n",
    "print(\"Model saved as:\", model_filename)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-19T12:15:43.022423Z"
    }
   },
   "id": "e0e155fdfdd5bb14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
